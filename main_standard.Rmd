---
title: "Statistical analysis"
output:
  html_notebook: default
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    latex_engine: lualatex
    toc: yes
---

<P style="page-break-before: always">

```{r}
path_loc = "D:/Onedrive/SNCF/Analyses/sncf_memoire_analysis/results/"
setwd(path_loc)
options(scipen=999)
rm(path_loc)
```
\
\

# I. **Introduction**
\
\
This file is used to produce all the analyses performed to validate the **General flexibility Scale** (**Code R** : **nflex** = new flexibility). It contains script to describe the sample (age, sexe...), items of the **nflex** scale,the exploratory factorial analysis (using **polychoric correlation** and **principal axis factor analysis**) and the confirmatory factorial analysis used to assess the internal structure of the other scale (**polychoric correlations** and **diagonal weighted least square factor analysis**).\
A simulated dataset with same parameters is available at : https://github.com/Anghille/flexibility_memory_analysis
\
\
\

# II. **Setup**
## 1. **Loading librairies**

```{r echo=T, message=FALSE, warning=FALSE, include=T}
wants <- c("psych","reshape","reshape2","car","carData","moments","tidyverse","stringr","plotly","readxl","GGally","grid","gridExtra","data.table","esquisse","Hmisc","psychTools","stargazer","polycor","lavaan","lavaanPlot","ggpubr","corrplot","outliers","GPArotation","jtools")

# IF R CRASHES, REMOVE "JTOOLS" FROM THE "wants" VECTOR AND INSTALL/CALL IT AFTER BY UNCOMMENTING THE LAST 2 LINES OF THIS CHUNK

has <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
invisible(lapply(wants, library, character.only = TRUE))
rm(has, wants)

# Uncomment it if needed (see last comment for the "why")
# install.packages("jtools")
# library(jtools)
```
\

## 2. **Loading database**
```{r}
# Change path were your dataset is downloaded
path = "D:/Onedrive/SNCF/Analyses/sncf_memoire_analysis/results/DATA.csv"
data = read.csv2(file = path)
rm(path)
head(data)
```
\

## 3. **database cleaning**

The goal here is to **automatically delete** subjects with missing data from one of the scale, or if there level of french isnt enough which might interfer with the french scale coprehension. It also normalize the strings of the dataset to lowercase and invert back the inverted items. there is 4 dataframe in total : 
* Raw data -- **data**
* Data with general flexibility items only -- **nflex**
* data with all scales cleaned of *NA* -- **data_cleaned_scale**
* Data with general flexibility item only and item inverted back -- **nflex_inv**


```{r}
# Rename some variables
x = 0
a = c("Situation","Mothertongue","French level","Pourcentage")
for(i in c("Situation.professionnelle.avant.le.confinement","Langue.maternelle","Niveau.de.franÃƒ.ais","Pourcentage_perseveration_error_count")){
  x = x+1
  names(data)[names(data)==i] = a[x]
}

# set column name to lowerstring and pourcentage column to numeric (will tranform empty cells to NA)
data = setnames(data, tolower(names(data)))
data$pourcentage = as.numeric(data$pourcentage)

# Create all the needed dataframes
data_cleaned_scale = data %>% drop_na(c(7:42)) %>% filter(!(situation=="Sans emploi")) #For scale analysis
data_cleaned_wcst = data %>% drop_na(c(7:48)) %>% filter(!(situation=="Sans emploi")) #For validation with wcst
nflex_inv = data_cleaned_scale[,c(21:30)] #For GCF (general flexibility scale) analysis

# Invert back item that are inverted in the scales
inverted_item_likert7 = c("cognitive_rigidity_1","att_switch_1","att_switch_2","att_switch_4","att_switch_9","att_switch_10","cfs_2","cfs_3","cfs_5","cfs_10","nflex_3","nflex_5","nflex_7","nflex_9")
inverted_item_likert4 = c("att_switch_1","att_switch_2","att_switch_4","att_switch_9","att_switch_10")
for(i in inverted_item_likert7){
    if(i %in% inverted_item_likert4){
      data_cleaned_scale[[i]] = 5-data_cleaned_scale[[i]]
      data_cleaned_wcst[[i]] = 5-data_cleaned_wcst[[i]]
    } else{
      data_cleaned_scale[[i]] = 7-data_cleaned_scale[[i]]
      data_cleaned_wcst[[i]] = 7-data_cleaned_wcst[[i]]
    }
}

#Remove unwanted variables
rm(a, inverted_item_likert4, inverted_item_likert7, x)
```

**Note**. *We'll be using nflex_inv for further analysis of the scale structure. We'll be using data_cleaned_scale for the confirmatory factor analysis of the other scales.* 
\
\

# III. **Descriptive statistics**
## 1. **Sample description**
```{r}
# Shows frequencies of variables
t = describe(data$situation)
as.data.frame(t$values)
describe(data$sexe)

# Shows descriptive table
stargazer(as.data.frame(data), type="text")

# All data set description with 350 subjects
stargazer(data_cleaned_scale, type="text")

# Sample Age description 
statss = data_cleaned_scale %>% 
  group_by(sexe, situation) %>% 
  summarise(mean_age = mean(age), sd_age = sd(age))
# v = c(3,4,6,7)
# for(i in 1:4){
#   statss[v[i], 1]=NA
# }
stargazer(as.data.frame(statss), type="text", summary = F, digits = 2)
rm(statss)

describe(data_cleaned_scale$situation)
describe(data_cleaned_scale$sexe)
```

\
<center> **Sexe and Situation distribution function of age :** </center>
```{r}
# Subject's histogram

data_ggplot = data
data_ggplot$situation = factor(data_ggplot$situation, levels=c("Etudiant", "Travailleur", "Sans emploi"))
data_ggplot$sexe = factor(data_ggplot$sexe, levels=c("Femme", "Homme", "Autre"))

## For sexe/age
ggplotly(ggplot(data=data_ggplot, aes(x=age, fill=sexe)) + 
  geom_histogram(color="#e9ecef", alpha=.9, position ="identity",binwidth = 2) + 
  scale_fill_manual(values=c("#404080", "#b8c2cc", "#69b3a2")) +
  theme_apa() +
  theme(legend.position = "right") +
  labs(fill="Sexe") +
  scale_x_continuous(name="Age", breaks = seq(0, 85, 10)) +
  ylab("N"))

## For situation/age
ggplotly(ggplot(data=data_ggplot, aes(x=age, fill=situation))+
  geom_histogram(color="#e9ecef", alpha=.9, position ="identity",binwidth = 2) + 
  scale_fill_manual(values=c("#404080", "#b8c2cc", "#69b3a2")) +
  theme_apa() +
  theme(legend.position = "right") +
  ylab("N") +
  labs(fill="Situation") +
  scale_x_continuous(name="Age", breaks = seq(0, 85, 10)))


data_ggplot = data_cleaned_scale
data_ggplot$situation = factor(data_ggplot$situation, levels=c("Etudiant", "Travailleur", "Sans emploi"))
data_ggplot$sexe = factor(data_ggplot$sexe, levels=c("Femme", "Homme", "Autre"))

## For sexe/age
ggplotly(ggplot(data=data_ggplot, aes(x=age, fill=sexe)) + 
  geom_histogram(color="#e9ecef", alpha=.9, position ="identity",binwidth = 2) + 
  scale_fill_manual(values=c("#404080", "#b8c2cc", "#69b3a2")) +
  theme_apa() +
  theme(legend.position = "right") +
  labs(fill="Sexe") +
  scale_x_continuous(name="Age", breaks = seq(0, 85, 10)) +
  ylab("N"))

## For situation/age
ggplotly(ggplot(data=data_ggplot, aes(x=age, fill=situation))+
  geom_histogram(color="#e9ecef", alpha=.9, position ="identity",binwidth = 2) + 
  scale_fill_manual(values=c("#404080", "#b8c2cc", "#69b3a2")) +
  theme_apa() +
  theme(legend.position = "right") +
  ylab("N") +
  labs(fill="Situation") +
  scale_x_continuous(name="Age", breaks = seq(0, 85, 10)))

rm(data_ggplot)
```
\
\


## 2. **General Cognitive Flexibility Scale**
### a. *Histograms*
 \
Shows histogram for each item of the general cognitive flexibility scale. The goal is to check if any item seems to have an odd distribution, which would indicate that he isn't suited to measure our flexibility. We therefore check item for **ceiling** or **skewed** items. 


```{r}
# Shows the general flexibility items' histograms
ggplot_data = nflex_inv
ggplot_data = ggplot_data %>% 
  rename(Q1 = nflex_1,
         Q2 = nflex_2,
         Q3 = nflex_3,
         Q4 = nflex_4,
         Q5 = nflex_5,
         Q6 = nflex_6,
         Q7 = nflex_7,
         Q8 = nflex_8,
         Q9 = nflex_9,
         Q10 = nflex_10)
ggplotly(ggplot(gather(ggplot_data), aes(x=value)) + 
  geom_histogram(stat = "count") + facet_wrap(~key, scales = 'free_x') +
  scale_color_grey() +
  theme_apa() +
  theme(legend.position = "bottom") +
  xlab("Questions") + ylab("N"))

ggplotly(ggplot(gather(data_cleaned_scale[,11:20]), aes(x=value)) + 
  geom_histogram(stat = "count") + facet_wrap(~key, scales = 'free_x') +
  scale_color_grey() +
  theme_apa() +
  theme(legend.position = "bottom") +
  xlab("Questions") + ylab("N"))

ggplotly(ggplot(gather(data_cleaned_scale[,7:10]), aes(x=value)) + 
  geom_histogram(stat = "count") + facet_wrap(~key, scales = 'free_x') +
  scale_color_grey() +
  theme_apa() +
  theme(legend.position = "bottom") +
  xlab("Questions") + ylab("N"))

ggplotly(ggplot(gather(data_cleaned_scale[,31:42]), aes(x=value)) + 
  geom_histogram(stat = "count") + facet_wrap(~key, scales = 'free_x') +
  scale_color_grey() +
  theme_apa() +
  theme(legend.position = "bottom") +
  xlab("Questions") + ylab("N"))

rm(ggplot_data)
```

**Items 1, 4, 6, 10** seems to have *ceiling* or *skewed* distribution. It is then recommended to check if **extreme values** of each item (values 1 and 6) have more than 20% of the responses. We therefore check for *skewness* and *kurtosis*. (Garin, 2014 ; Benzina, 2019)\
\
**Reminder **:\
The skewness coefficient is a third-order measure of the central tendency of the sample. A negative coefficient translate to a biased distribution toward the right. A positive coefficient translate to a biased distribution toward the left. 
$$\gamma_{X} = E\left[\left(\frac{X - \mu}{\sigma}\right)^3\right] $$ \
\
The kurtosis coefficient is a 4th order measure of the central tendency of the sample. It translate to how fast the distribution goes back to 0 (if it does). Therefore, it shows if there is more extreme values than it should. A high kurtosis translate to high number of extreme values.  
$$Kurt[X] = E\left[\left(\frac{X - \mu}{\sigma}\right)^4\right] $$ \
\

```{r warning=FALSE}
# Compute skewness and Kurtosis
item = colnames(nflex_inv)
kurt_skew = as.tibble(round(skewness(nflex_inv), 2)) %>% 
  mutate(kurtosis = round(kurtosis(nflex_inv), 2)) %>% 
  cbind(item) %>% 
  rename(skewness = value)
rm(item)
kurt_skew[, c(3,1,2)]

rm(kurt_skew)
```
\
\
*References*

Benzina, N. (2019). Ã‰valuation de la flexibilitÃ© cognitive dans le trouble obsessionnel compulsifâ€¯: Ã‰tude de la validitÃ© de deux auto-questionnaires comparÃ©s Ã  une tÃ¢che expÃ©rimentale. UniversitÃ© de Rouen.

\
The **table below** shows items with more than 20% of extreme values. 
```{r}
# This script check what item got more than 20% of response in the extreme values. It then create a new dataframe containing only the items non extreme. 

## Create dataframe with upper item and lower item frequencies of response
frequencies  = data.frame(item = character(10), upper = numeric(10), lower = numeric(10))
for(i in 1:10){
  upper = length(which(nflex_inv[i]==6))/nrow(nflex_inv)
  lower = length(which(nflex_inv[i]==1))/nrow(nflex_inv)
  frequencies$item[i] = paste0("nflex_", as.character(i))
  frequencies$upper[i] = upper
  frequencies$lower[i] = lower
}
frequencies
rm(upper, lower, i)

## Create a list used to delete item that have more than 20% of extreme values.
list = c()
for(i in 1:10){
  if(frequencies[i,2]>.20 | frequencies[i,3]>.20 ){
    list = c(list, i)
    }
}
# Create a dataframe with the remaining items of the General Cognitive Flexibility (GCF)
nflex_inv_reduct = nflex_inv[,-list]
rm(list,i)
```
\

### b. *Summary*
\
\
Items 1, 4, 6, 10 are in fact not well suited. We deleted them from further analysis.The next table shows the flexibiliy descriptive stats for the remaining items
```{r}
# Shows descriptive statistique for remaining items of the GCF
stargazer(as.data.frame(nflex_inv_reduct), type="text")
```
\

### c. *correlation* 
\
\
We show here the item's correlation. We used **polychoric correlation** as it is suited for ordinal values with skewed distribution. The polychoric correlation is a measure of the pairwise association of ordinal variables. We smooth the correlation to avoid any non positive definite measure (Debelak et Tran, 2016)
\
\
*RÃ©fÃ©rences*

Debelak, R., & Tran, U. S. (2016). Comparing the Effects of Different Smoothing Algorithms on the Assessment of Dimensionality of Ordered Categorical Items with Parallel Analysis. *PloS one, 11(2)*, e0148143. https://doi.org/10.1371/journal.pone.0148143

D.L. Knol and JMF ten Berge (1989) Least squares approximation of an improper correlation matrix by a proper one. *Psychometrika, 54*, 53-61.

```{r message=FALSE, warning=FALSE}
# Compute the polychoric correlation (rho), smooth it by eigenvalue decomposition
poly_nflex_inv_reduct = polychoric(nflex_inv_reduct)
poly_nflex_inv_reduct = poly_nflex_inv_reduct$rho
poly_nflex_inv_reduct = cor.smooth(poly_nflex_inv_reduct, eig.tol=10^-12)
```

```{r}
# Shows correlation in a matrix
correlation = poly_nflex_inv_reduct
correlation[upper.tri(correlation)] <- NA
stargazer(correlation, title="Polychloric correlation matrix", type="text")
rm(correlation)
```
\

# IV. **Exploratory Factorial Analysis**
## 1. **EFA's assumption**
\
\
```{r, include=T}
# Compute bartlett, KMO and check if we got an identity matrix
cortest.bartlett(R = poly_nflex_inv_reduct, n = nrow(nflex_inv_reduct))
KMO(poly_nflex_inv_reduct)
det(poly_nflex_inv_reduct)
```
\
<center> **KMO's measure of sample adequacy test** </center>
$$MSA_{nflex} = .63 \space (MSA_{min} = .60)$$ \

<center> **Bartlett Sphericity test** </center>
$$\chi^2(6) = 205.59,\space p < .001.$$
<center> *EFA is appropriate.* </center> \
\
<center>**Det(cor(A))** </center>
$$det(A) = 0.55,\space non-identity\space matrix$$
\
As shown byt the KMO test, items 8 and 9 seems to have a really low KMO. We therefore deleted them from further analysis, computed the polychoric correlation of this new dataset and tested again the assumptions :
```{r, include=T}
# Compute new dataset of nflex without item 8 and 9
nflex_inv_reduct_msa = nflex_inv_reduct[,-c(5,6)]

#Compute polychoric correlation with the item 8 and 9 removed
poly_nflex_inv_reduct_msa = polychoric(nflex_inv_reduct_msa)
poly_nflex_inv_reduct_msa = poly_nflex_inv_reduct_msa$rho
poly_nflex_inv_reduct_msa = cor.smooth(poly_nflex_inv_reduct_msa, eig.tol=10^-12)

# Compute bartlett, KMO and check if we got an identity matrix
cortest.bartlett(R = poly_nflex_inv_reduct_msa, n = nrow(nflex_inv_reduct_msa))
KMO(poly_nflex_inv_reduct_msa)
det(poly_nflex_inv_reduct_msa)

rm(poly_nflex_inv_reduct)
```
\
<center> **KMO's measure of sample adequacy test** </center>
$$MSA_{nflex} = .68 \space (MSA_{min} = .60)$$ \

<center> **Bartlett Sphericity test** </center>
$$\chi^2(6) = 110.51,\space p < .001.$$
<center> *EFA is appropriate.* </center> \
\
<center>**Det(cor(A))** </center>
$$det(A) = 0.73,\space non-identity\space matrix$$ 
\
We need to test if it's recommanded and possible to do an EFA. The **Bartlett Sphericity Test** compare our data matrix with matrix randomly generated by those same date and then check if those data are linked. The **measure sample adequacy test** of Kaiser-Meyer and Olin check sample adequacy for each variable in the model and for the complete.\
\
**Reminder** : *Common variance* (variance shared between an item and other items), *specific variance* (variance of the item that isnt shared with other items) et *residual variance* (error associated to the item) \
**Reminder 2** : Because we have ordinal data, we need to use the **polychoric correlation matrix** in the KMO and Bartlett tests and not the raw dataset, otherwise it will automatically use the **pearson correlation matrix** !
**Reminder 3** : MSA rise as sample size rise, mean correlation rise, number of variables rise et land number of factors drops. We used the modified MSA (Kaiser et Rice, 1974).
\

*RÃ©fÃ©rences* 

Kaiser, H. F., & Rice, J. (1974). Little Jiffy, Mark Iv. *Educational and Psychological Measurement, 34(1)*, 111â€‘117. https://doi.org/10.1177/001316447403400115

Tinsley, H. E. A., & Tinsley, D. J. (1987). Uses of factor analysis in counseling psychology research. *Journal of Counseling Psychology, 34(4)*, 414â€‘424. https://doi.org/0022-0167.34.4.414

\
\

#### *Compute RÂ² of items*
```{r}
# Compute multiple regression for RÂ² 
lm1 = lm(nflex_2~nflex_3+nflex_5+nflex_7, data=nflex_inv_reduct_msa)
#ConditionRegression(lm1)
lm2 = lm(nflex_3~nflex_2+nflex_5+nflex_7, data=nflex_inv_reduct_msa)
#ConditionRegression(lm2)
lm3 = lm(nflex_5~nflex_3+nflex_2+nflex_7, data=nflex_inv_reduct_msa)
#ConditionRegression(lm3)
lm4 = lm(nflex_7~nflex_3+nflex_5+nflex_2, data=nflex_inv_reduct_msa)
#ConditionRegression(lm4)

# summary(lm1)
# summary(lm2)
# summary(lm3)
# summary(lm4)

#RÂ² obtained with those regressions 
c(.09796, .1277,.1005,.1609)
```

## 2. **Polychoric exploratory factorial analysis**
We used the **parallel analysis with polychoric correlation** because we have ordinal data, skewed items and non normality:
 

\
\
*RÃ©fÃ©rence*

Buja, A., & Eyuboglu, N. (1992). Remarks on Parallel Analysis. *Multivariate Behavioral Research, 27(4)*, 509-540. http://doi.org/10.1207/s15327906mbr2704_2

Devlin, S. J., Gnanadesikan, R., & Kettenring, J. R. (1981). Robust estimation of dispersion matrices and principal components. *Journal of the American Statistical Association, 76*, 354-362. http://doi.org/10.1080/01621459.1981.10477654

ten Berge, J. M. F., & Kiers, H. A. L. (1991). A numerical approach to the approximate and the exact minimum rank of a covariance matrix. *Psychometrika, 56(2)*, 309-315. http://doi.org/10.1007/BF02294464

Timmerman, M. E., & Lorenzo-Seva, U. (2011). Dimensionality assessment of ordered polytomous items with parallel analysis. *Psychological Methods, 16(2)*, 209-220. http://doi.org/10.1037/a0023353

Ford, J. Kevin, Robert C. MacCALLUM, et Marianne Tait. Â«Â The Application of Exploratory Factor Analysis in Applied Psychology: A Critical Review and AnalysisÂ Â». *Personnel Psychology 39, náµ’ 2* (juin 1986): 291â€‘314. https://doi.org/10.1111/j.1744-6570.1986.tb00583.x. (critÃ¨re du 0.40) \

Garrido, L. E., Abad, F. J., & Ponsoda, V. (2013). A new look at Hornâ€™s parallel analysis with ordinal variables. *Psychological Methods, 18(4)*, 454â€‘474. PubMed. https://doi.org/10.1037/a0030005 \

Tinsley, H. E. A., & Tinsley, D. J. (1987). Uses of factor analysis in counseling psychology research. *Journal of Counseling Psychology, 34(4)*, 414â€‘424. https://doi.org/0022-0167.34.4.414
\
\
We then used the efa wiith **least squares algorithm** (*principal axis factor analysis*), with number of factor indicated by the parallel analysis, and an oblimin rotation. We set the factor loadings limit to .40 (Peterson, 2000). Anything below is considered too small. 
\
\
*RÃ©fÃ©rence*

Ford, J. K., MacCALLUM, R. C., & Tait, M. (1986). The application of exploratory factor analysis in applied psychologyâ€¯: A critical review and analysis. *Personnel Psychology, 39(2)*, 291â€‘314. https://doi.org/10.1111/j.1744-6570.1986.tb00583.x \

Lee, S.-Y., Poon, W.-Y., & Bentler, P. M. (1995). A two-stage estimation of structural equation models with continuous and polytomous variables. *British Journal of Mathematical and Statistical Psychology, 48(2)*, 339â€‘358. https://doi.org/10.1111/j.2044-8317.1995.tb01067.x \

Baglin, J. (2014). Improving Your Exploratory Factor Analysis for Ordinal Dataâ€¯: A Demonstration Using FACTOR. *Practical Assessment, Research, and Evaluation*, 19(5), 1â€‘16. https://doi.org/10.7275/dsep-4220 \

Peterson, R. A. (2000). A Meta-Analysis of Variance Accounted for and Factor Loadings in Exploratory Factor Analysis. *Marketing Letters, 11(3)*, 261â€‘275.\

Tinsley, H. E. A., & Tinsley, D. J. (1987). Uses of factor analysis in counseling psychology research. *Journal of Counseling Psychology, 34(4)*, 414â€‘424. https://doi.org/0022-0167.34.4.414\


```{r}
# Compute the number of factors
pa = fa.parallel(poly_nflex_inv_reduct_msa, fm="pa", fa="fa", main = "Scree Plot", se.bars = T, n.obs = nrow(nflex_inv_reduct_msa))
vss(poly_nflex_inv_reduct_msa)

# EFA with polychoric correlation and RÂ² from multiple regression as original communalitites
fit_nflex = fa(nflex_inv_reduct_msa, cor="poly", nfactors = pa$nfact, fm="pa", rotate="none", residuals = T, correct=F, SMC=c(.09796, .1277,.1005,.1609))
fa.diagram(fit_nflex, sort = F, errors = T, labels = T, digits = 2, cut=.39)
fa.plot(fit_nflex) #plot the loadings of each factors
fit_nflex

# Reliability Test
fit_omega_nflex = omega(m=nflex_inv_reduct_msa, poly=T, nfactors = 1, fm = "pa", rotation="none", plot = T)
```
\

Fit index of the EFA shows how well the model is doing compare to a base model (all item in one factor). It also shows the fitting of items on the factors. We need to check for the **$\chiÂ²$ test** (results close to 0 shows a perfect fit), the **root meant square residuals** (range between 0 and 1), the **standardized root mean residual** (SRMR), the **Tucker Lewis index** (TLI). \
\
The **$\chiÂ²$ test** is used for hypothesis testing to evaluate the appropriateness of a structural equation model. It checks if the sample covariance matrix $S$ is equal to the model-implied covariance matrix $\Sigma (\theta)$ (Null hypothesis :$S-\Sigma (\hat{\theta})=0$). The **$\chiÂ²$ test** is sensitive to number of observation. He will always be significant with more than 400 observations. (Schermelleh-Engel et al., 2003).\
\
Because exact fit never occurs, the null hypothesis of exact fit is replaced by the null hypothesis of "close-fit". Thus, the **RMSEA** is a measure of approximate fit in the population and is therefore concerned with the discrepancy due to approximation. (Schermelleh-Engel et al., 2003). Steiger (1990) and Browne and Cudeck (1993) define a "close-fit" as a  $RMSEA \leq .05$, an adequate fit as a  $.05\leq RMSEA  \leq.08$, a mediocre fit as $.08\leq RMSEA  \leq.10$ and anythin else as not acceptable. For Hu and Bentler (1999), a cutoff of .06 is appropriate. RMSEA is relatively iundependent of sample size and favors parsimonious models (Browne and Cudeck, 1993 ; Kaplan, 2000).\
\
The **standardized root mean square residual** (SRMR) was developped to overcome the problems that comes along with the root mean residual, which is dependant on the siezs of the variance and covariances of the observed variables.A value of 0 indicates a perfect fit. But there is not real cutoff, as this value is still dependent of variance and covariances of the observed variables (even if less than for the RMR). Hu and Bentler (1995) suggested that $SRMR \leq .05$ indicate a good fit and $.05\leq SRMR  \leq.10$ indicates an acceptable fit.\
\
The **Turker Lewis Index**, also known as **NonNormed Fit index** (Bentler and Bonnett, 1980) ranges from 0 to 1 but can sometime go beyond 1, as it is nonnormed. Less restrictive models (more complexe) are penalized while more persimonious models are rewarded by an increase in the fit index. This index is one of the less affected by sample size (Bentler, 1990 ; Hu et Bentler, 1998 ; Schermelleh-Engel et al., 2003)
\
\

*RÃ©fÃ©rences* :

Sharma, S., Mukherjee, S., Kumar, A., & Dillon, W. R. (2005). A simulation study to investigate the use of cutoff values for assessing model fit in covariance structure models. *Journal of Business Research, 58(7)*, 935â€‘943. https://doi.org/10.1016/j.jbusres.2003.10.007\

Bagozzi, R. R., & Yi, Y. (1988). On the evaluation of structural equation models. *Journal of the Academy of Marketing Science, 16(1)*, 74â€‘94. https://doi.org/0092-0703/88 / 1601-0074\

Hu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysisâ€¯: Conventional criteria versus new alternatives. *Structural Equation Modeling: A Multidisciplinary Journal, 6(1)*, 1â€‘55. https://doi.org/10.1080/10705519909540118\

Schermelleh-Engel, K., Moosbrugger, H., & MÃ¼ller, H. (2003). Evaluating the Fit of Structural Equation Modelsâ€¯: Tests of Significance and Descriptive Goodness-of-Fit Measures. *Methods of Psychological Research, 8(2)*, 23â€‘74.

```{r}
knitr::kable(data.frame("alpha" = fit_omega_nflex$alpha, "Omega"= fit_omega_nflex$omega.tot)) %>% kableExtra::kable_styling("striped", full_width = F, position="left")
```


The Omega is an estimation of the general factor saturation in a scale. The Omega asymptotic coefficient can be compare to a Guttman $\lambda^6$ (or the Cronbach $\alpha$).
\
\
*RÃ©fÃ©rences* 

Revelle and Zinbarg (2009)

Zinbarg, R. E., Revelle, W., Yovel, I., & Li, W. (2005). Cronbachâ€™s Î±, Revelleâ€™s Î², and Mcdonaldâ€™s Ï‰Hâ€¯: Their relations with each other and two alternative conceptualizations of reliability. *Psychometrika, 70(1)*, 123â€‘133. https://doi.org/10.1007/s11336-003-0974-7

Trizano-Hermosilla, I., & Alvarado, J. M. (2016). Best Alternatives to Cronbachâ€™s Alpha Reliability in Realistic Conditionsâ€¯: Congeneric and Asymmetrical Measurements. *Frontiers in Psychology, 7(769)*. https://doi.org/10.3389/fpsyg.2016.00769


# V. **Confirmatory Factor Analysis** 
## 1. **Creation of cfa models**

Create models used for the cfa. It implies the 3 scales already validated (cfs, attention switching sub-set, cognitive rigidity sub-set). 
We used DWLS method as it is suited for ordinal data (diagnonally weighter least square). We got the same index as for the EFA plus the CFI (comparative fit index), the GFI (goodness of fit index) and AGFI (adjusted goodness of fit index). 

```{r}
#Create the lavaan models
model_cfs = "flexibilite =~ cfs_1+cfs_2+cfs_3+cfs_4+cfs_5+cfs_6+cfs_7+cfs_8+cfs_9+cfs_10+cfs_11+cfs_12"

model_aq = "attention =~ att_switch_1+att_switch_2+att_switch_3+att_switch_4+att_switch_5+att_switch_6+att_switch_7+att_switch_8+att_switch_9+att_switch_10"

model_rtc = "rigidite =~ cognitive_rigidity_1+cognitive_rigidity_2+cognitive_rigidity_3+cognitive_rigidity_4"
```
\

## 2. **CFA for the cognitive flexibility scale**

```{r}
cfa_data = data_cleaned_scale[,31:42]
#Compute CFA
cfa_cfs = cfa(model_cfs, data=cfa_data, ordered = c("cfs_1","cfs_2","cfs_3","cfs_4","cfs_5","cfs_6","cfs_7","cfs_8","cfs_9","cfs_10","cfs_11","cfs_12"))
summary(cfa_cfs, fit.measures=T)
fitmeasures(cfa_cfs)
lavaanPlot(model = cfa_cfs)
lavaan.diagram(cfa_cfs)

# test with efa stucture
##compute polychoric correlation and assumptions
poly = polychoric(data_cleaned_scale[,31:42])
poly = poly$rho
cortest.bartlett(R = poly, n = nrow(data_cleaned_scale))
KMO(poly)
det(poly)

##Compute parallel analysis for number of factors
pa = fa.parallel(poly, fm="pa", fa="fa", main="Scree Plot", se.bars=T, n.obs=nrow(data_cleaned_scale))

# EFA with polychoric correlation and RÂ² from multiple regression as original communalitites
fit_nflex = fa(data_cleaned_scale[,31:42], cor="poly", nfactors=pa$nfact, fm="pa", rotate="oblimin", residuals=T, correct=T, SMC=T)
fa.diagram(fit_nflex, sort=T, errors=T, labels=T, digits=2)
fa.plot(fit_nflex) #plot the loadings of each factors
fit_nflex

#Check if a general factor can explain the 3 factors
bassAckward(poly, cor="poly", nfactors = c(1,3), fm = "pa")

##Compute alpha and omega for internal reliability
psych::alpha(data_cleaned_scale[,31:42])
omega(poly, nfactors = 6, fm = "pa", sl=T, rotation="oblimin")

#Remove useless variables
rm(pa, poly, fit_nflex, cfa_cfs, model_cfs)
```
\

## 3. **CFA for the attention switching sub-scale**

```{r}

cfa_data = data_cleaned_scale[,11:20]
#Compute CFA
cfa_aq = cfa(model_aq, data=cfa_data, estimator="DWLS", ordered = c("att_switch_1","att_switch_2","att_switch_3","att_switch_4","att_switch_5","att_switch_6","att_switch_7","att_switch_8","att_switch_9","att_switch_10"))
fitmeasures(cfa_aq)
summary(cfa_aq, fit.measures=T)

# test with efa stucture
##compute polychoric correlation and assumptions
poly = polychoric(data_cleaned_scale[,c(11:13,15:20)])
poly = poly$rho
cortest.bartlett(R = poly, n = nrow(data_cleaned_scale))
KMO(poly)
det(poly)

##Compute parallel analysis for number of factors
pa = fa.parallel(poly, fm="pa", fa="fa", main = "Scree Plot", se.bars = T, n.obs = nrow(data_cleaned_scale))

# EFA with polychoric correlation and RÂ² from multiple regression as original communalitites
fit_nflex = fa(data_cleaned_scale[,11:20], cor="poly", nfactors = pa$nfact, fm="pa", rotate="varimax", residuals = T, correct=F, SMC=T)
fa.diagram(fit_nflex, sort = T, errors = T, labels = T, digits = 2)
fa.plot(fit_nflex) #plot the loadings of each factors
fit_nflex

#Check if a general factor can explain the 3 factors
bassAckward(poly, cor="poly", nfactors = c(1,3), fm = "pa")

##Compute alpha and omega for internal reliability
psych::alpha(data_cleaned_scale[,11:20])
omega(poly, nfactors = 4, fm = "pa", sl=T, rotation="none")

#Remove useless variables
rm(pa, poly, fit_nflex, cfa_aq, model_aq)
```
\


## 4. **CFA for the cognitive rigidity sub-scale**

```{r}
cfa_data = data_cleaned_scale[,7:10]
#Compute CFA
cfa_rtc = cfa(model_rtc, data=cfa_data, estimator="DWLS", ordered=c("cognitive_rigidity_1","cognitive_rigidity_2","cognitive_rigidity_3","cognitive_rigidity_4") )
summary(cfa_rtc, fit.measures=T)
fitmeasures(cfa_rtc)

#Compute alpha and omega)
poly = polychoric(data_cleaned_scale[,7:10])
poly = poly$rho
psych::alpha(data_cleaned_scale[,7:10])
omega(poly, nfactors = 1, fm = "pa", sl=T, rotation="none")

#Remove useless variables
rm(poly, cfa_rtc, model_rtc)
```

# VI. **Scale validation**
## 1. **Sum Computation**

```{r}
#Create columns with sum of each item by factors
data_cleaned_wcst$sum_nflex = apply(data_cleaned_wcst[,c("nflex_2","nflex_3","nflex_5","nflex_7")], 1, sum)
data_cleaned_scale$sum_nflex = apply(data_cleaned_scale[,c("nflex_2","nflex_3","nflex_5","nflex_7")], 1, sum)
data_cleaned_scale$sum_as = apply(data_cleaned_scale[,c(11:20)], 1, sum)
data_cleaned_scale$sum_cr = apply(data_cleaned_scale[,c(7:10)], 1, sum)
data_cleaned_scale$sum_cfs = apply(data_cleaned_scale[,c(31:42)], 1, sum)

#Compute outliers for wcst using boxplot
outliers<-which(data_cleaned_wcst$pourcentage %in% c(boxplot.stats(data_cleaned_wcst$pourcentage)$out))
nflex_wcst = data_cleaned_wcst[-c(outliers),]
```



## 2. **Correlations**

CFA didnt provide a proof of 1-factor construct for each of the scales. Even tho' we used EFA to assess the real construct of those scales, we cannot use them. IT would need an entire study to assess the veracity of thoses analysis. Furthermore, too many factors are found, with little theorical background to justify them. And some factors only got 1 item. Therefore, we used the same construct as declared by the author of the scales (which mean : 1 factor and therefore 1 sum score of each question)

```{r}
# PLot of scatterpoint with regression and loess lines (loess is used in timeseries analysis for exemple, and also to check if the relation between 2 variables is linear or not. A linear regression could be used where a quadratic relation fit best to the data ! )
# Add ggplotly(ggplot variable) to get an interactive graphic. Exemple : ggplotly(a)
a = ggplot(data_cleaned_scale, aes(x=sum_nflex, y=sum_cr))+
  geom_point(alpha=.2)+
  geom_smooth(method="lm", se=F, color="#13a4ba")+
  geom_smooth(method="loess", se=F, color="#1334ba")+
  stat_cor(label.x = 21, label.y = 30, digits = 2) +
  theme_apa() +
  xlab("GCF") +
  ylab("RTC-RC")

b = ggplot(data_cleaned_scale, aes(x=sum_nflex, y=sum_as))+
  geom_point(alpha=.2)+
  geom_smooth(method="lm", se=F, color="#13a4ba")+
  geom_smooth(method="loess", se=F, color="#1334ba")+
  stat_cor(label.x = 21, label.y = 40, digits = 2) +
  theme_apa() +
  xlab("GCF") +
  ylab("AQ-AS")

c = ggplot(data_cleaned_scale, aes(x=sum_nflex, y=sum_cfs))+
  stat_cor(label.x = 21, label.y = 80, digits = 2) +
  geom_point(alpha=.2)+
  geom_smooth(method="lm", se=F, color="#13a4ba")+
  geom_smooth(method="loess", se=F, color="#1334ba")+
  theme_apa() +
  xlab("GCF") +
  ylab("CFS")

d = ggplot(nflex_wcst, aes(x=sum_nflex, y=pourcentage))+
  geom_point(alpha=.2)+
  geom_smooth(method="lm", se=F, color="#13a4ba")+
  geom_smooth(method="loess", se=F, color="#1334ba")+
  stat_cor(label.x = 20, label.y = 30, digits = 2) +
  theme_apa() +
  xlab("GCF") +
  ylab("WCST (%)")

figure = ggarrange(d,c,b,a, ncol=2, nrow=2, label.x="GCF")
figure
```

Here, you can find the correlation between our scale and the scales/cognitive tasks used in the study. 

```{r}
rcorr(x = nflex_wcst$pourcentage, y=nflex_wcst$sum_nflex)
```


```{r}
rcorr(x = data_cleaned_scale$sum_as, y=data_cleaned_scale$sum_nflex)
```


```{r}
rcorr(x = data_cleaned_scale$sum_cr, y=data_cleaned_scale$sum_nflex)
```


```{r}
rcorr(x = data_cleaned_scale$sum_cfs, y=data_cleaned_scale$sum_nflex)
```

```{r}
rcorr(x = nflex_wcst$pourcentage, y=nflex_wcst$sum_cfs)
```






















